{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'plotting'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5835f9254fb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\samue\\Anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;31m# deprecate tools.plotting, plot_params and scatter_matrix on the top namespace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotting\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m \u001b[0mplot_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_style\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;31m# do not import deprecate to top namespace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m scatter_matrix = pandas.util._decorators.deprecate(\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'plotting'"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10788"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load 10k reuters news documents \n",
    "len(reuters.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\\n  Mounting trade friction between the\\n  U.S. And Japan has raised fears among many of Asia's exporting\\n  nations that the row could inflict far-reachin\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view text from one document \n",
    "reuters.raw(fileids=['test/14826'])[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ASIAN EXPORTERS FEAR DAMAGE FROM USJAPAN RIFT\\n  Mounting trade friction between the\\n  US And Japan h'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove punctuation example\n",
    "\n",
    "## remove punctutation \n",
    "texttest = reuters.raw(fileids=['test/14826'])\n",
    "exclude = set(string.punctuation)\n",
    "texttest2 = texttest\n",
    "texttest2 = ''.join(ch for ch in texttest2 if ch not in exclude)\n",
    "texttest2[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHINA DAILY SAYS VERMIN EAT 712 PCT GRAIN STOCKS\n",
      "  A survey of 19 provinces and seven cities\n",
      "  showed vermin consume between seven and 12 pct of Chinas grain\n",
      "  stocks the China Daily said\n",
      "      It also said that each year 1575 mln tonnes or 25 pct of\n",
      "  Chinas fruit output are left to rot and 21 mln tonnes or up\n",
      "  to 30 pct of its vegetables The paper blamed the waste on\n",
      "  inadequate storage and bad preservation methods\n",
      "      It said the government had launched a national programme to\n",
      "  reduce waste calling for improved technology in storage and\n",
      "  preservation and greater production of additives The paper\n",
      "  gave no further details\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove punctation all DOCs \n",
    "exclude = set(string.punctuation)\n",
    "alldocslist = []\n",
    "\n",
    "for index, i in  enumerate(reuters.fileids()):\n",
    "    text = reuters.raw(fileids=[i])\n",
    "    text = ''.join(ch for ch in text if ch not in exclude)\n",
    "    alldocslist.append(text)\n",
    "    \n",
    "print(alldocslist[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASIAN',\n",
       " 'EXPORTERS',\n",
       " 'FEAR',\n",
       " 'DAMAGE',\n",
       " 'FROM',\n",
       " 'U.S.-JAPAN',\n",
       " 'RIFT',\n",
       " 'Mounting',\n",
       " 'trade',\n",
       " 'friction']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize EXAMPLE (need to go back to deal with apostraphies etc.)\n",
    "\n",
    "texttest = reuters.raw(fileids=['test/14826'])\n",
    "\n",
    "#tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#tokentest = tokenizer.tokenize(texttest)\n",
    "\n",
    "#pattern = r'''\\.\\.\\.'''\n",
    "\n",
    "#tokentest = nltk.regexp_tokenize(texttest, pattern)\n",
    "\n",
    "\n",
    "#tokentest[0:100]\n",
    "#texttest\n",
    "\n",
    "tokentest = word_tokenize(texttest)\n",
    "tokentest[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10788"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alldocslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenize ALL DOCS \n",
    "plot_data = [[]] * len(alldocslist)\n",
    "\n",
    "for doc in alldocslist:\n",
    "    text = doc\n",
    "    tokentext = word_tokenize(text)\n",
    "    plot_data[index].append(tokentext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DAILY'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_data[0][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHINA',\n",
       " 'DAILY',\n",
       " 'SAYS',\n",
       " 'VERMIN',\n",
       " 'EAT',\n",
       " '712',\n",
       " 'PCT',\n",
       " 'GRAIN',\n",
       " 'STOCKS',\n",
       " 'A']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second index gives document \n",
    "plot_data[0][1][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['china', 'daily', 'says', 'vermin', 'eat', '712', 'pct', 'grain', 'stocks', 'a']\n"
     ]
    }
   ],
   "source": [
    "#make all lower case EXAMPLE \n",
    "tokentest = plot_data[0][1]\n",
    "tokentestlower = []\n",
    "for word in tokentest:\n",
    "    word = word.lower()\n",
    "    tokentestlower.append(word)\n",
    "\n",
    "print(tokentestlower[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['china',\n",
       " 'daily',\n",
       " 'says',\n",
       " 'vermin',\n",
       " 'eat',\n",
       " '712',\n",
       " 'pct',\n",
       " 'grain',\n",
       " 'stocks',\n",
       " 'a']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make all lower case ALL DOCS\n",
    "for x in range(len(reuters.fileids())):\n",
    "    lowers = [word.lower() for word in plot_data[0][x]]\n",
    "    plot_data[0][x] = lowers\n",
    "\n",
    "plot_data[0][1][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['china',\n",
       " 'daily',\n",
       " 'says',\n",
       " 'vermin',\n",
       " 'eat',\n",
       " '712',\n",
       " 'pct',\n",
       " 'grain',\n",
       " 'stocks',\n",
       " 'survey']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stop words EXAMPLE \n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sentence = [w for w in tokentestlower if not w in stop_words]\n",
    "\n",
    "filtered_sentence[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['china',\n",
       " 'daily',\n",
       " 'says',\n",
       " 'vermin',\n",
       " 'eat',\n",
       " '712',\n",
       " 'pct',\n",
       " 'grain',\n",
       " 'stocks',\n",
       " 'survey']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stop words ALL DOCS\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for x in range(len(reuters.fileids())):\n",
    "    filtered_sentence = [w for w in plot_data[0][x] if not w in stop_words]\n",
    "    plot_data[0][x] = filtered_sentence\n",
    "\n",
    "plot_data[0][1][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ltaha',\n",
       " 'automot',\n",
       " 'technolog',\n",
       " 'corp',\n",
       " 'year',\n",
       " 'net',\n",
       " 'shr',\n",
       " '43',\n",
       " 'ct',\n",
       " 'vs']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stem words EXAMPLE (could try others/lemmers )\n",
    "\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "stemmed_sentence = [snowball_stemmer.stem(w) for w in filtered_sentence]\n",
    "stemmed_sentence[0:10]\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "stemmed_sentence = [ porter_stemmer.stem(w) for w in filtered_sentence]\n",
    "stemmed_sentence[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stem words ALL DOCS\n",
    "#snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "#for x in range(len(reuters.fileids())):\n",
    "#    stemmed_sentence = [snowball_stemmer.stem(w) for w in plot_data[0][x]]\n",
    "#    plot_data[0][x] = stemmed_sentence\n",
    "\n",
    "#plot_data[0][1][0:10]\n",
    "\n",
    "#### NEEED TO COME BACK AND STEM THIS PROPERLY !!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create inverse index which gives document number for each document where word appears\n",
    "\n",
    "#create list of all words \n",
    "l = plot_data[0]\n",
    "flatten = [item for sublist in l for item in sublist]\n",
    "words = flatten\n",
    "wordsunique = set(words)\n",
    "wordsunique = list(wordsunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0004668944867571099"
      ]
     },
     "execution_count": 883,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf \n",
    "tester = plot_data[0][0:-1]\n",
    "\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "def tf(word, doc):\n",
    "    return doc.count(word) / len(doc)\n",
    "\n",
    "def n_containing(word, doclist):\n",
    "    return sum(1 for doc in doclist if word in doc)\n",
    "\n",
    "def idf(word, doclist):\n",
    "    return math.log(len(doclist) / (0.1 + n_containing(word, doclist)))\n",
    "\n",
    "def tfidf(word, doc, doclist):\n",
    "    return (tf(word, doc) * idf(word, doclist))\n",
    "\n",
    "\n",
    "plottest = plot_data[0][1:10]\n",
    "tfidf('said',plottest[0],plottest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dictonary of words\n",
    "# THIS ONE-TIME INDEXING IS THE MOST PROCESSOR-INTENSIVE STEP AND WILL TAKE TIME TO RUN (BUT ONLY NEEDS TO BE RUN ONCE)\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "plottest = plot_data[0][0:-1]\n",
    "\n",
    "worddic = {}\n",
    "\n",
    "for doc in plottest:\n",
    "    for word in wordsunique:\n",
    "        if word in doc:\n",
    "            word = str(word)\n",
    "            index = plottest.index(doc)\n",
    "            positions = list(np.where(np.array(plottest[index]) == word)[0])\n",
    "            idfs = tfidf(word,doc,plottest)\n",
    "            try:\n",
    "                worddic[word].append([index,positions,idfs])\n",
    "            except:\n",
    "                worddic[word] = []\n",
    "                worddic[word].append([index,positions,idfs])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, [0, 23], 0.11298148846131757],\n",
       " [12, [0], 0.06684738067294622],\n",
       " [13, [160], 0.013193561974923598],\n",
       " [27, [51], 0.058128157106909766],\n",
       " [39, [3, 15, 59, 79], 0.14718689322483575],\n",
       " [235, [86], 0.04407519604809642],\n",
       " [280, [70], 0.056490744230658786],\n",
       " [292, [13, 21], 0.11625631421381953],\n",
       " [301, [33], 0.05986332597577274],\n",
       " [341, [55, 146], 0.053836816649352665],\n",
       " [566, [2], 0.06915246276511679],\n",
       " [568, [1014, 1072, 1221], 0.009234480829723961],\n",
       " [611, [20], 0.019954441991924247],\n",
       " [709, [0, 7, 34], 0.1743844713207293],\n",
       " [719, [0, 16], 0.23593193178686905],\n",
       " [720, [0, 6, 27, 78, 82], 0.2025678202210492],\n",
       " [732, [179], 0.021563671184821366],\n",
       " [735, [0, 5, 21, 83], 0.1371228321496333]]"
      ]
     },
     "execution_count": 873,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worddic['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(639, 11),\n",
       " (395, 10),\n",
       " (563, 9),\n",
       " (567, 9),\n",
       " (570, 9),\n",
       " (583, 9),\n",
       " (431, 8),\n",
       " (599, 8),\n",
       " (693, 8)]"
      ]
     },
     "execution_count": 874,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create simply word search which takes a word and finds the documents that contain it \n",
    "\n",
    "word = 'tonnes'\n",
    "doclist = []\n",
    "enddic = {}\n",
    "\n",
    "for pair in worddic[word]:\n",
    "    index = pair[0]\n",
    "    amount = len(pair[1])\n",
    "    enddic[index] = amount\n",
    "    fullcount_order = sorted(enddic.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "fullcount_order[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(32, 13),\n",
       "  (620, 12),\n",
       "  (33, 11),\n",
       "  (208, 8),\n",
       "  (122, 7),\n",
       "  (659, 5),\n",
       "  (356, 5),\n",
       "  (609, 5),\n",
       "  (694, 5),\n",
       "  (723, 5),\n",
       "  (747, 5),\n",
       "  (3, 4),\n",
       "  (213, 4),\n",
       "  (425, 4),\n",
       "  (568, 4),\n",
       "  (5, 3),\n",
       "  (310, 3),\n",
       "  (692, 3),\n",
       "  (109, 3),\n",
       "  (296, 3),\n",
       "  (536, 3),\n",
       "  (571, 3),\n",
       "  (661, 3),\n",
       "  (869, 3),\n",
       "  (919, 3),\n",
       "  (9, 2),\n",
       "  (265, 2),\n",
       "  (267, 2),\n",
       "  (271, 2),\n",
       "  (363, 2),\n",
       "  (761, 2),\n",
       "  (866, 2),\n",
       "  (1, 2),\n",
       "  (2, 2),\n",
       "  (22, 2),\n",
       "  (186, 2),\n",
       "  (216, 2),\n",
       "  (288, 2),\n",
       "  (291, 2),\n",
       "  (293, 2),\n",
       "  (306, 2),\n",
       "  (443, 2),\n",
       "  (753, 2),\n",
       "  (948, 2),\n",
       "  (43, 1),\n",
       "  (280, 1),\n",
       "  (39, 1),\n",
       "  (696, 1),\n",
       "  (183, 1),\n",
       "  (18, 1),\n",
       "  (51, 1),\n",
       "  (90, 1),\n",
       "  (185, 1),\n",
       "  (194, 1),\n",
       "  (203, 1),\n",
       "  (204, 1),\n",
       "  (256, 1),\n",
       "  (258, 1),\n",
       "  (266, 1),\n",
       "  (290, 1),\n",
       "  (383, 1),\n",
       "  (403, 1),\n",
       "  (427, 1),\n",
       "  (497, 1),\n",
       "  (552, 1),\n",
       "  (581, 1),\n",
       "  (595, 1),\n",
       "  (639, 1),\n",
       "  (699, 1),\n",
       "  (740, 1),\n",
       "  (763, 1),\n",
       "  (776, 1),\n",
       "  (783, 1),\n",
       "  (799, 1),\n",
       "  (810, 1),\n",
       "  (812, 1),\n",
       "  (913, 1),\n",
       "  (934, 1),\n",
       "  (989, 1),\n",
       "  (996, 1)],\n",
       " [(3, 1.0),\n",
       "  (5, 1.0),\n",
       "  (208, 0.5),\n",
       "  (280, 0.5),\n",
       "  (32, 0.5),\n",
       "  (213, 0.5),\n",
       "  (267, 0.5),\n",
       "  (271, 0.5),\n",
       "  (620, 0.5),\n",
       "  (659, 0.5),\n",
       "  (692, 0.5),\n",
       "  (696, 0.5),\n",
       "  (33, 0.5),\n",
       "  (723, 0.5),\n",
       "  (9, 0.25),\n",
       "  (43, 0.25),\n",
       "  (310, 0.25),\n",
       "  (39, 0.25),\n",
       "  (265, 0.25),\n",
       "  (363, 0.25),\n",
       "  (183, 0.25),\n",
       "  (761, 0.25),\n",
       "  (866, 0.25),\n",
       "  (1, 0.25),\n",
       "  (2, 0.25),\n",
       "  (18, 0.25),\n",
       "  (22, 0.25),\n",
       "  (51, 0.25),\n",
       "  (90, 0.25),\n",
       "  (109, 0.25),\n",
       "  (122, 0.25),\n",
       "  (185, 0.25),\n",
       "  (186, 0.25),\n",
       "  (194, 0.25),\n",
       "  (203, 0.25),\n",
       "  (204, 0.25),\n",
       "  (216, 0.25),\n",
       "  (256, 0.25),\n",
       "  (258, 0.25),\n",
       "  (266, 0.25),\n",
       "  (288, 0.25),\n",
       "  (290, 0.25),\n",
       "  (291, 0.25),\n",
       "  (293, 0.25),\n",
       "  (296, 0.25),\n",
       "  (306, 0.25),\n",
       "  (356, 0.25),\n",
       "  (383, 0.25),\n",
       "  (403, 0.25),\n",
       "  (425, 0.25),\n",
       "  (427, 0.25),\n",
       "  (443, 0.25),\n",
       "  (497, 0.25),\n",
       "  (536, 0.25),\n",
       "  (552, 0.25),\n",
       "  (568, 0.25),\n",
       "  (571, 0.25),\n",
       "  (581, 0.25),\n",
       "  (595, 0.25),\n",
       "  (609, 0.25),\n",
       "  (639, 0.25),\n",
       "  (661, 0.25),\n",
       "  (694, 0.25),\n",
       "  (699, 0.25),\n",
       "  (740, 0.25),\n",
       "  (747, 0.25),\n",
       "  (753, 0.25),\n",
       "  (763, 0.25),\n",
       "  (776, 0.25),\n",
       "  (783, 0.25),\n",
       "  (799, 0.25),\n",
       "  (810, 0.25),\n",
       "  (812, 0.25),\n",
       "  (869, 0.25),\n",
       "  (913, 0.25),\n",
       "  (919, 0.25),\n",
       "  (934, 0.25),\n",
       "  (948, 0.25),\n",
       "  (989, 0.25),\n",
       "  (996, 0.25)],\n",
       " [(761, 0.48556819118755123),\n",
       "  (265, 0.4334142548441448),\n",
       "  (866, 0.3398977338312859),\n",
       "  (288, 0.23897279403251365),\n",
       "  (213, 0.18776433816840357),\n",
       "  (425, 0.18776433816840357),\n",
       "  (536, 0.17922959552438522),\n",
       "  (43, 0.17666642837861726),\n",
       "  (33, 0.17419101251767563),\n",
       "  (694, 0.1685064573306186),\n",
       "  (109, 0.1677894085760202),\n",
       "  (9, 0.16488866648670944),\n",
       "  (620, 0.1569373572750836),\n",
       "  (356, 0.15283143804404942),\n",
       "  (661, 0.14879438119005567),\n",
       "  (310, 0.13999981116796087),\n",
       "  (427, 0.1314350367178825),\n",
       "  (919, 0.11770301795631269),\n",
       "  (296, 0.11429133627641956),\n",
       "  (208, 0.1035941176101537),\n",
       "  (3, 0.09558911761300545),\n",
       "  (363, 0.0902946364258635),\n",
       "  (692, 0.08047043064360153),\n",
       "  (291, 0.07965759801083788),\n",
       "  (934, 0.07965759801083788),\n",
       "  (32, 0.07610937538229276),\n",
       "  (747, 0.07597400966351589),\n",
       "  (90, 0.07510573526736143),\n",
       "  (659, 0.07383990826847331),\n",
       "  (609, 0.069176335114675),\n",
       "  (869, 0.06683137460231314),\n",
       "  (216, 0.06571751835894125),\n",
       "  (267, 0.06042990193925632),\n",
       "  (306, 0.055929802858673404),\n",
       "  (497, 0.055929802858673404),\n",
       "  (403, 0.04694108454210089),\n",
       "  (723, 0.04611755674311666),\n",
       "  (1, 0.04532242645444224),\n",
       "  (2, 0.04532242645444224),\n",
       "  (266, 0.043811678905960834),\n",
       "  (185, 0.043093454661600826),\n",
       "  (913, 0.043093454661600826),\n",
       "  (186, 0.04239839894125242),\n",
       "  (39, 0.03976277567377475),\n",
       "  (581, 0.0392343393187709),\n",
       "  (948, 0.0392343393187709),\n",
       "  (812, 0.038097112092139854),\n",
       "  (22, 0.03782303214903094),\n",
       "  (280, 0.03702395400503732),\n",
       "  (271, 0.03676504523577133),\n",
       "  (293, 0.03676504523577133),\n",
       "  (696, 0.03504934312476867),\n",
       "  (290, 0.03413897057607338),\n",
       "  (122, 0.03357829405201378),\n",
       "  (5, 0.03092589099244294),\n",
       "  (256, 0.02767053404587),\n",
       "  (799, 0.024799063531675943),\n",
       "  (740, 0.024339821614422684),\n",
       "  (194, 0.02305877837155833),\n",
       "  (753, 0.02227712486743771),\n",
       "  (204, 0.019328681870276837),\n",
       "  (763, 0.017642286807769463),\n",
       "  (203, 0.016028663014375916),\n",
       "  (443, 0.01574072296022545),\n",
       "  (18, 0.0156470281807003),\n",
       "  (989, 0.015372518914372222),\n",
       "  (996, 0.014057223178383154),\n",
       "  (183, 0.013892277949507597),\n",
       "  (776, 0.013078113106256966),\n",
       "  (258, 0.012949264701269213),\n",
       "  (639, 0.012003199700263242),\n",
       "  (699, 0.01128197740067661),\n",
       "  (810, 0.011044961068729622),\n",
       "  (51, 0.009882333587810713),\n",
       "  (383, 0.009455758037257735),\n",
       "  (552, 0.009455758037257735),\n",
       "  (568, 0.008069687595879203),\n",
       "  (783, 0.008014331507187958),\n",
       "  (595, 0.005267937343402104),\n",
       "  (571, 0.005236455646130777)],\n",
       " [(33, 6),\n",
       "  (3, 5),\n",
       "  (659, 5),\n",
       "  (5, 4),\n",
       "  (267, 2),\n",
       "  (692, 2),\n",
       "  (213, 2),\n",
       "  (620, 1),\n",
       "  (32, 0),\n",
       "  (696, 0),\n",
       "  (271, 0),\n",
       "  (208, 0),\n",
       "  (723, 0),\n",
       "  (280, 0)])"
      ]
     },
     "execution_count": 879,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create more complex word search which takes multiple words and finds documents that contain both \n",
    "from collections import Counter\n",
    "\n",
    "def search(searchsentence):\n",
    "    try:\n",
    "        # split sentence into individual words \n",
    "        words = searchsentence.split(' ')\n",
    "        enddic = {}\n",
    "        idfdic = {}\n",
    "        closedic = {}\n",
    "        \n",
    "        # remove words if not in worddic \n",
    "        realwords = []\n",
    "        for word in words:\n",
    "            if word in list(worddic.keys()):\n",
    "                realwords.append(word)  \n",
    "        words = realwords\n",
    "        numwords = len(words)\n",
    "        \n",
    "        # make metric of number of occurances of all words in each doc & largest total IDF \n",
    "        for word in words:\n",
    "            for indpos in worddic[word]:\n",
    "                index = indpos[0]\n",
    "                amount = len(indpos[1])\n",
    "                idfscore = indpos[2]\n",
    "                enddic[index] = amount\n",
    "                idfdic[index] = idfscore\n",
    "                fullcount_order = sorted(enddic.items(), key=lambda x:x[1], reverse=True)\n",
    "                fullidf_order = sorted(idfdic.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "                \n",
    "        # make metric of what percentage of words appear in each doc\n",
    "        combo = []\n",
    "        alloptions = {k: worddic.get(k, None) for k in (words)}\n",
    "        for worddex in list(alloptions.values()):\n",
    "            for indexpos in worddex:\n",
    "                for indexz in indexpos:\n",
    "                    combo.append(indexz)\n",
    "        comboindex = combo[::3]\n",
    "        combocount = Counter(comboindex)\n",
    "        for key in combocount:\n",
    "            combocount[key] = combocount[key] / numwords\n",
    "        combocount_order = sorted(combocount.items(), key=lambda x:x[1], reverse=True)\n",
    "        \n",
    "        # make metric for if words appear in same order as in search\n",
    "        x = []\n",
    "        y = []\n",
    "        for record in [worddic[z] for z in words]:\n",
    "            for index in record:\n",
    "                 x.append(index[0])\n",
    "        for i in x:\n",
    "            if x.count(i) > 1:\n",
    "                y.append(i)\n",
    "        y = list(set(y))\n",
    "        \n",
    "        closedic = {}\n",
    "        for wordbig in [worddic[x] for x in words]:\n",
    "            for record in wordbig:\n",
    "                if record[0] in y:\n",
    "                    index = record[0]\n",
    "                    positions = record[1]\n",
    "                    try:\n",
    "                        closedic[index].append(positions)\n",
    "                    except:\n",
    "                        closedic[index] = []\n",
    "                        closedic[index].append(positions)\n",
    "\n",
    "        x = 0\n",
    "        fdic = {}\n",
    "        for index in y:\n",
    "            csum = []\n",
    "            for seqlist in closedic[index]:\n",
    "                while x > 0:\n",
    "                    secondlist = seqlist\n",
    "                    x = 0\n",
    "                    sol = [1 for i in firstlist if i + 1 in secondlist]\n",
    "                    csum.append(sol)\n",
    "                    fsum = [item for sublist in csum for item in sublist]\n",
    "                    fsum = sum(fsum)\n",
    "                    fdic[index] = fsum\n",
    "                    fdic_order = sorted(fdic.items(), key=lambda x:x[1], reverse=True)\n",
    "                while x == 0:\n",
    "                    firstlist = seqlist\n",
    "                    x = x + 1\n",
    "                    \n",
    "        # also the one above should be given a big boost if ALL found together \n",
    "           \n",
    "        \n",
    "        #could make another metric for if they are not next to each other but still close \n",
    "        \n",
    "        \n",
    "        return(fullcount_order,combocount_order,fullidf_order,fdic_order)\n",
    "    \n",
    "    except:\n",
    "        return(\"\")\n",
    "\n",
    "\n",
    "search('indonesia crude palm oil')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(182, 5), (39, 4), (568, 3), (709, 3), (341, ...</td>\n",
       "      <td>[(0, 1.0), (12, 0.6666666666666666), (13, 0.66...</td>\n",
       "      <td>[(674, 0.5090802962710302), (134, 0.4363545396...</td>\n",
       "      <td>[(0, 3), (292, 1), (719, 1), (720, 1), (12, 0)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(32, 13), (620, 12), (33, 11), (208, 8), (122...</td>\n",
       "      <td>[(3, 1.0), (5, 1.0), (208, 0.5), (280, 0.5), (...</td>\n",
       "      <td>[(761, 0.48556819118755123), (265, 0.433414254...</td>\n",
       "      <td>[(33, 6), (3, 5), (659, 5), (5, 4), (267, 2), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [(182, 5), (39, 4), (568, 3), (709, 3), (341, ...   \n",
       "1  [(32, 13), (620, 12), (33, 11), (208, 8), (122...   \n",
       "\n",
       "                                                   1  \\\n",
       "0  [(0, 1.0), (12, 0.6666666666666666), (13, 0.66...   \n",
       "1  [(3, 1.0), (5, 1.0), (208, 0.5), (280, 0.5), (...   \n",
       "\n",
       "                                                   2  \\\n",
       "0  [(674, 0.5090802962710302), (134, 0.4363545396...   \n",
       "1  [(761, 0.48556819118755123), (265, 0.433414254...   \n",
       "\n",
       "                                                   3  \n",
       "0  [(0, 3), (292, 1), (719, 1), (720, 1), (12, 0)...  \n",
       "1  [(33, 6), (3, 5), (659, 5), (5, 4), (267, 2), ...  "
      ]
     },
     "execution_count": 880,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result1 = search('china daily says what')\n",
    "result2 = search('indonesia crude palm oil')\n",
    "pd.DataFrame([result1,result2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Once the input query has been expanded, it’s executed\n",
    "#• The search engine will access the inverted index to fetch those documents\n",
    "#  that contain the words, and produce a result set\n",
    "\n",
    "#• Ranking is sorting the results in order of relevance\n",
    "#• It’s based on the input query as well as the documument content\n",
    "\n",
    "# • There are many methods to perform ranking on the document result set\n",
    "# • In web search, there’s lots of knowledge that can be applied, mostly from\n",
    "#information that is inferred from user behavior\n",
    "# • In offline information retrieval, this is simpler and less accurate because\n",
    "#there’s less sources of information\n",
    "\n",
    "#• The main signal that is used for ranking is tf-idf\n",
    "#• TF: term frequency\n",
    "#• IDF: inverse document frequency\n",
    "\n",
    "#• TF is defined as the number of times that a word appears in a given\n",
    "#document\n",
    "#• The more times it appears, the higher the score\n",
    "\n",
    "#• IDF is defined as the inverse of the number of documents that the word\n",
    "#appears in\n",
    "#• The less documents, the higher the score\n",
    "#• It is logarithmically scaled\n",
    "\n",
    "#• tf-idf is then defined as the product of tf and idf\n",
    "#• For a given word (or term), document and collection of documents\n",
    "#• There are different weighting schemes that can be defined as slight\n",
    "# variations of the general formula\n",
    "\n",
    "#• The answer is to apply a ranking function based on the tf-idfs of individual\n",
    "#words in the query\n",
    "#• The most popular one is BM25 (aka Okapi BM25), and its variants such as\n",
    "#BM25F, BM25+, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deal with punctuation\n",
    "\n",
    "#tokentestlower[0:100]\n",
    "\n",
    "# 'u.s.-japan',\n",
    "# 'asia',\n",
    "# \"'s\",\n",
    "    \n",
    "#    'tokyo',\n",
    "# \"'s\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create dataframe of each\n",
    "df = pd.DataFrame({'words': words})\n",
    "df[\"docs\"] = \"\"\n",
    "df2 = df.drop_duplicates()\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THINGS TO DO\n",
    "\n",
    "# create inverse index with document number and position of word \n",
    "# NEED TO MAKE SURE IT RETURNS MULTIPLE IF CONTAINS MULTIPLE \n",
    "# create search engine to look up documents containing words\n",
    "# create query expansion ability for search engine \n",
    "\n",
    "## NEED TO ADD IDF for number of occurances metric\n",
    "## need to make sure that if a dud word is put in then it does not kill search \n",
    "\n",
    "## also add weight if these words are in the same order as the search\n",
    "\n",
    "### YOU DON'T WANT POSTION BUT WORD POSITION!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get corpus of many documents\n",
    "# Tokenize \n",
    "# make lower case \n",
    "# deal with punctuation\n",
    "# Remove stopwords\n",
    "# Lematize/Stemm words \n",
    "\n",
    "# Create inverse index which gives document number for each document where word appears\n",
    "# also reference the place in the document it is found \n",
    "\n",
    "## give search and it fetch docs that contain  words from the inverted index \n",
    "## gives weight to words that are next to each other / in the same worder \n",
    "\n",
    "# Can also add word expansion \n",
    "# synonyms, related words\n",
    "# This means system generates super query that contains more terms than initial\n",
    "\n",
    "# And apply TD-IDF / BM25+\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
